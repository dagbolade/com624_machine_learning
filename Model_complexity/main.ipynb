{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(699, 11)\n",
      "(699, 10)\n",
      "\n",
      " The total of training dataset (489, 9)\n",
      "\n",
      " The total of test dataset (210, 9)\n",
      "Model accuracy on Train data: 1.00 \n",
      "\n",
      "Model accuracy on Test data: 0.95 \n",
      "\n",
      "The Confusion Matrix: \n",
      " [[134   4]\n",
      " [  7  65]] \n",
      "\n",
      "Report of classification: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         2.0       0.95      0.97      0.96       138\n",
      "         4.0       0.94      0.90      0.92        72\n",
      "\n",
      "    accuracy                           0.95       210\n",
      "   macro avg       0.95      0.94      0.94       210\n",
      "weighted avg       0.95      0.95      0.95       210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from matplotlib import pyplot\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.model_selection import  train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#declaring header names\n",
    "winsconsin_headers = ['sample_code','c_thickness', 'uni_cell_size', 'uni_cell_shape', 'marg_adhesion', 'epi_cell_size', 'nuclei','bland_chromatin', 'normal_nucleoli', 'mitoses', 'tumor_class']\n",
    "\n",
    "#read the data using read_csv class of pandas\n",
    "wins_data = read_csv(\"venv/winsconsin_b_cancer (1).csv\" ,names= winsconsin_headers)\n",
    "\n",
    "print(wins_data.shape)\n",
    "\n",
    "wins_data.drop('sample_code', axis=1, inplace=True)\n",
    "print(wins_data.shape)\n",
    "#check all datas are numbers and convert any non-numeric characters to null value\n",
    "wins_data=wins_data.apply(pd.to_numeric, errors='coerce')\n",
    "#print(wins_data.apply(pd.to_numeric, errors='coerce').info()) #this will give the datatype info after the conversion\n",
    "\n",
    "#declaring a new header\n",
    "new_winsconsin_headers = ['c_thickness', 'uni_cell_size', 'uni_cell_shape', 'marg_adhesion', 'epi_cell_size', 'nuclei','bland_chromatin', 'normal_nucleoli', 'mitoses', 'tumor_class']\n",
    "\n",
    "#since ? cannot be converted to int we can convert all data to float\n",
    "wins_data[new_winsconsin_headers] = wins_data[new_winsconsin_headers].applymap(float)\n",
    "#print(wins_data.dtypes)\n",
    "\n",
    "#use the simple imputer function to replace missing value\n",
    "imputer = SimpleImputer (strategy = 'median') # replace most_frequent with median, mean and observe\n",
    "imputer.fit(wins_data)\n",
    "new_data = imputer.transform(wins_data)\n",
    "#reassign the new data frame\n",
    "wins_data = pd.DataFrame(new_data, columns=new_winsconsin_headers)\n",
    "\n",
    "#recheck the data for missing values\n",
    "win_empty_data = wins_data[wins_data.isna().any(axis=1)]\n",
    "#print('\\n These are the missing data \\n ', win_empty_data)\n",
    "\n",
    "#seperate the data into xtrain and y test groups  - training and target sets\n",
    "train_headers = ['c_thickness', 'uni_cell_size', 'uni_cell_shape', 'marg_adhesion', 'epi_cell_size', 'nuclei','bland_chromatin', 'normal_nucleoli', 'mitoses']\n",
    "target_header = ['tumor_class']\n",
    "\n",
    "X = wins_data[train_headers]\n",
    "y = wins_data[target_header]\n",
    "#split the data into train and test -- split  using 60:40\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1,stratify=y)\n",
    "\n",
    "#check the dimension of the train and test data\n",
    "print('\\n The total of training dataset', X_train.shape)\n",
    "print('\\n The total of test dataset', X_test.shape)\n",
    "\n",
    "#instantiate the model\n",
    "my_model = DecisionTreeClassifier()\n",
    "\n",
    "#train the model to fit\n",
    "my_model.fit(X_train, y_train)\n",
    "#now let's predict the model\n",
    "y_pred_train = my_model.predict(X_train)\n",
    "\n",
    "y_pred = my_model.predict(X_test)\n",
    "\n",
    "#compute the train accuracy\n",
    "model_acc = accuracy_score(y_train, y_pred_train)\n",
    "print(\"Model accuracy on Train data: {:.2f}\".format(model_acc), '\\n')\n",
    "\n",
    "\n",
    "#compute test set accuracy\n",
    "model_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model accuracy on Test data: {:.2f}\".format(model_accuracy), '\\n')\n",
    "\n",
    "#constructing a confusion matrix  of the test data\n",
    "matrix_info = confusion_matrix(y_test, y_pred)\n",
    "print(\"The Confusion Matrix: \\n\", matrix_info, '\\n')\n",
    "\n",
    "#construct the classification report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Report of classification: \\n\", class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "    Tuning the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(699, 11)\n",
      "(699, 10)\n",
      "\n",
      " The total of training dataset (629, 9)\n",
      "\n",
      " The total of test dataset (70, 9)\n",
      "Model accuracy on Train data: 0.97 \n",
      "\n",
      "Model accuracy on Test data: 0.89 \n",
      "\n",
      "The Confusion Matrix: \n",
      " [[41  5]\n",
      " [ 3 21]] \n",
      "\n",
      "Report of classification: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         2.0       0.93      0.89      0.91        46\n",
      "         4.0       0.81      0.88      0.84        24\n",
      "\n",
      "    accuracy                           0.89        70\n",
      "   macro avg       0.87      0.88      0.88        70\n",
      "weighted avg       0.89      0.89      0.89        70\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from matplotlib import pyplot\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.model_selection import  train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#declaring header names\n",
    "winsconsin_headers = ['sample_code','c_thickness', 'uni_cell_size', 'uni_cell_shape', 'marg_adhesion', 'epi_cell_size', 'nuclei','bland_chromatin', 'normal_nucleoli', 'mitoses', 'tumor_class']\n",
    "\n",
    "#read the data using read_csv class of pandas\n",
    "wins_data = read_csv(\"venv/winsconsin_b_cancer (1).csv\" ,names= winsconsin_headers)\n",
    "\n",
    "print(wins_data.shape)\n",
    "\n",
    "wins_data.drop('sample_code', axis=1, inplace=True)\n",
    "print(wins_data.shape)\n",
    "#check all datas are numbers and convert any non-numeric characters to null value\n",
    "wins_data=wins_data.apply(pd.to_numeric, errors='coerce')\n",
    "#print(wins_data.apply(pd.to_numeric, errors='coerce').info()) #this will give the datatype info after the conversion\n",
    "\n",
    "#declaring a new header\n",
    "new_winsconsin_headers = ['c_thickness', 'uni_cell_size', 'uni_cell_shape', 'marg_adhesion', 'epi_cell_size', 'nuclei','bland_chromatin', 'normal_nucleoli', 'mitoses', 'tumor_class']\n",
    "\n",
    "#since ? cannot be converted to int we can convert all data to float\n",
    "wins_data[new_winsconsin_headers] = wins_data[new_winsconsin_headers].applymap(float)\n",
    "#print(wins_data.dtypes)\n",
    "\n",
    "#use the simple imputer function to replace missing value\n",
    "imputer = SimpleImputer (strategy = 'median') # replace most_frequent with median, mean and observe\n",
    "imputer.fit(wins_data)\n",
    "new_data = imputer.transform(wins_data)\n",
    "#reassign the new data frame\n",
    "wins_data = pd.DataFrame(new_data, columns=new_winsconsin_headers)\n",
    "\n",
    "#recheck the data for missing values\n",
    "win_empty_data = wins_data[wins_data.isna().any(axis=1)]\n",
    "#print('\\n These are the missing data \\n ', win_empty_data)\n",
    "\n",
    "#seperate the data into xtrain and y test groups  - training and target sets\n",
    "train_headers = ['c_thickness', 'uni_cell_size', 'uni_cell_shape', 'marg_adhesion', 'epi_cell_size', 'nuclei','bland_chromatin', 'normal_nucleoli', 'mitoses']\n",
    "target_header = ['tumor_class']\n",
    "\n",
    "X = wins_data[train_headers]\n",
    "y = wins_data[target_header]\n",
    "\n",
    "#split the data into train and test -- split  using 60:40\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1,stratify=y)\n",
    "\n",
    "#check the dimension of the train and test data\n",
    "print('\\n The total of training dataset', X_train.shape)\n",
    "print('\\n The total of test dataset', X_test.shape)\n",
    "\n",
    "#declare a seed variable to ensure reproducibility\n",
    "SEED =1\n",
    "#instantiate the model and set the hyperparameters\n",
    "my_model = DecisionTreeClassifier(max_depth=6, min_samples_leaf=0.01, random_state=SEED)#manually tuning our model\n",
    "\n",
    "#train the model to fit\n",
    "my_model.fit(X_train, y_train)\n",
    "#now let's predict the model\n",
    "y_pred_train = my_model.predict(X_train)\n",
    "\n",
    "y_pred = my_model.predict(X_test)\n",
    "\n",
    "#compute the train accuracy\n",
    "model_acc = accuracy_score(y_train, y_pred_train)\n",
    "print(\"Model accuracy on Train data: {:.2f}\".format(model_acc), '\\n')\n",
    "\n",
    "\n",
    "#compute test set accuracy\n",
    "model_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model accuracy on Test data: {:.2f}\".format(model_accuracy), '\\n')\n",
    "\n",
    "#constructing a confusion matrix  of the test data\n",
    "matrix_info = confusion_matrix(y_test, y_pred)\n",
    "print(\"The Confusion Matrix: \\n\", matrix_info, '\\n')\n",
    "\n",
    "#construct the classification report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Report of classification: \\n\", class_report)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "    Generalisation Error - overfitting and cv"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(699, 11)\n",
      "(699, 10)\n",
      "\n",
      " The total of training dataset (419, 9)\n",
      "\n",
      " The total of test dataset (280, 9)\n",
      "\n",
      " Cross val mean: 0.945 (std: 0.026)\n",
      "\n",
      "Model accuracy on Train data: 0.97 \n",
      "\n",
      "Model accuracy on Test data: 0.95 \n",
      "\n",
      "The Confusion Matrix: \n",
      " [[177   6]\n",
      " [  9  88]] \n",
      "\n",
      "Report of classification: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         2.0       0.95      0.97      0.96       183\n",
      "         4.0       0.94      0.91      0.92        97\n",
      "\n",
      "    accuracy                           0.95       280\n",
      "   macro avg       0.94      0.94      0.94       280\n",
      "weighted avg       0.95      0.95      0.95       280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from matplotlib import pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.model_selection import  train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "#declaring header names\n",
    "winsconsin_headers = ['sample_code','c_thickness', 'uni_cell_size', 'uni_cell_shape', 'marg_adhesion', 'epi_cell_size', 'nuclei','bland_chromatin', 'normal_nucleoli', 'mitoses', 'tumor_class']\n",
    "\n",
    "#read the data using read_csv class of pandas\n",
    "wins_data = read_csv(\"venv/winsconsin_b_cancer (1).csv\" ,names= winsconsin_headers)\n",
    "\n",
    "print(wins_data.shape)\n",
    "\n",
    "wins_data.drop('sample_code', axis=1, inplace=True)\n",
    "print(wins_data.shape)\n",
    "#check all datas are numbers and convert any non-numeric characters to null value\n",
    "wins_data=wins_data.apply(pd.to_numeric, errors='coerce')\n",
    "#print(wins_data.apply(pd.to_numeric, errors='coerce').info()) #this will give the datatype info after the conversion\n",
    "\n",
    "#declaring a new header\n",
    "new_winsconsin_headers = ['c_thickness', 'uni_cell_size', 'uni_cell_shape', 'marg_adhesion', 'epi_cell_size', 'nuclei','bland_chromatin', 'normal_nucleoli', 'mitoses', 'tumor_class']\n",
    "\n",
    "#since ? cannot be converted to int we can convert all data to float\n",
    "wins_data[new_winsconsin_headers] = wins_data[new_winsconsin_headers].applymap(float)\n",
    "#print(wins_data.dtypes)\n",
    "\n",
    "#use the simple imputer function to replace missing value\n",
    "imputer = SimpleImputer (strategy = 'median') # replace most_frequent with median, mean and observe\n",
    "imputer.fit(wins_data)\n",
    "new_data = imputer.transform(wins_data)\n",
    "#reassign the new data frame\n",
    "wins_data = pd.DataFrame(new_data, columns=new_winsconsin_headers)\n",
    "\n",
    "#recheck the data for missing values\n",
    "win_empty_data = wins_data[wins_data.isna().any(axis=1)]\n",
    "#print('\\n These are the missing data \\n ', win_empty_data)\n",
    "\n",
    "#seperate the data into xtrain and y test groups  - training and target sets\n",
    "train_headers = ['c_thickness', 'uni_cell_size', 'uni_cell_shape', 'marg_adhesion', 'epi_cell_size', 'nuclei','bland_chromatin', 'normal_nucleoli', 'mitoses']\n",
    "target_header = ['tumor_class']\n",
    "\n",
    "X = wins_data[train_headers]\n",
    "y = wins_data[target_header]\n",
    "\n",
    "#split the data into train and test -- split  using 60:40\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1,stratify=y)\n",
    "\n",
    "#check the dimension of the train and test data\n",
    "print('\\n The total of training dataset', X_train.shape)\n",
    "print('\\n The total of test dataset', X_test.shape)\n",
    "\n",
    "#declare a seed variable to ensure reproducibility\n",
    "SEED =1\n",
    "#instantiate the model and set the hyperparameters\n",
    "#my_model = DecisionTreeClassifier(max_depth=4, min_samples_leaf=0.05, random_state=SEED, criterion = 'gini')#manually tuning our model\n",
    "#my_model = DecisionTreeClassifier()\n",
    "#the best hyperparameters\n",
    "my_model = DecisionTreeClassifier(max_depth=4, min_samples_leaf=1, random_state=SEED, criterion = 'entropy', min_samples_split=8)\n",
    "#train the model to fit\n",
    "my_model.fit(X_train, y_train)\n",
    "\n",
    "#declare the num of folds\n",
    "num_folds = KFold(n_splits=10, random_state=1, shuffle=True) # test b changing splits to 3, 5 and 10\n",
    "\n",
    "#compute the array containing the 10 folds and calculate the cross validation mean score\n",
    "CV_scores = -cross_val_score(my_model, X_train, y_train, cv=num_folds)\n",
    "#std for the computation range for this generalisation and mean to know how well the model generalises\n",
    "print(\"\\n Cross val mean: {:.3f} (std: {:.3f})\".format(CV_scores.mean()*-1, CV_scores.std()), end=\"\\n\\n\")\n",
    "\n",
    "#now let's predict the model for training set\n",
    "y_pred_train = my_model.predict(X_train)\n",
    "\n",
    "#now let predict the model for the test set\n",
    "y_pred = my_model.predict(X_test)\n",
    "\n",
    "#compute the train accuracy\n",
    "model_acc = accuracy_score(y_train, y_pred_train)\n",
    "print(\"Model accuracy on Train data: {:.2f}\".format(model_acc), '\\n')\n",
    "\n",
    "\n",
    "#compute test set accuracy\n",
    "model_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model accuracy on Test data: {:.2f}\".format(model_accuracy), '\\n')\n",
    "\n",
    "#constructing a confusion matrix  of the test data\n",
    "matrix_info = confusion_matrix(y_test, y_pred)\n",
    "print(\"The Confusion Matrix: \\n\", matrix_info, '\\n')\n",
    "\n",
    "#construct the classification report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Report of classification: \\n\", class_report)\n",
    "\n",
    "#ploting the decision tree model\n",
    "#tree_classif_label = ['2', '4']\n",
    "#fig = plt.figure(figsize=(30,20))\n",
    "#tree.plot_tree(my_model, feature_names=train_headers, class_names=tree_classif_label, filled=True, rounded=True, fontsize=14)\n",
    "#fig.savefig('decisiontree.png') #save the image"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tuning techniques to get the best hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "GRID SEARCH CROSS VALIDATION TUNING"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(699, 11)\n",
      "(699, 10)\n",
      "\n",
      " The total of training dataset (419, 9)\n",
      "\n",
      " The total of test dataset (280, 9)\n",
      "\n",
      " Hyperparameters of default model \n",
      " {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': 1, 'splitter': 'best'} \n",
      "\n",
      "\n",
      " Cross val mean: 0.935 (std: 0.026)\n",
      "\n",
      "Best hyperparameters: {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 1, 'min_samples_split': 8}\n",
      "Best max_depth: =  4\n",
      "Best min_samples_split: =  8\n",
      "Best min_samples_leaf: =  1\n",
      "Best criterion: =  entropy\n",
      "suggested Best hyperparameters: \n",
      " {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 4, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 8, 'min_weight_fraction_leaf': 0.0, 'random_state': 1, 'splitter': 'best'}\n",
      "Best score: %s 0.944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Grid search model tuning\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from matplotlib import pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.model_selection import  train_test_split, KFold, cross_val_score,GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "#declaring header names\n",
    "winsconsin_headers = ['sample_code','c_thickness', 'uni_cell_size', 'uni_cell_shape', 'marg_adhesion', 'epi_cell_size', 'nuclei','bland_chromatin', 'normal_nucleoli', 'mitoses', 'tumor_class']\n",
    "\n",
    "#read the data using read_csv class of pandas\n",
    "wins_data = read_csv(\"venv/winsconsin_b_cancer (1).csv\" ,names= winsconsin_headers)\n",
    "\n",
    "print(wins_data.shape)\n",
    "\n",
    "wins_data.drop('sample_code', axis=1, inplace=True)\n",
    "print(wins_data.shape)\n",
    "#check all datas are numbers and convert any non-numeric characters to null value\n",
    "wins_data=wins_data.apply(pd.to_numeric, errors='coerce')\n",
    "#print(wins_data.apply(pd.to_numeric, errors='coerce').info()) #this will give the datatype info after the conversion\n",
    "\n",
    "#declaring a new header\n",
    "new_winsconsin_headers = ['c_thickness', 'uni_cell_size', 'uni_cell_shape', 'marg_adhesion', 'epi_cell_size', 'nuclei','bland_chromatin', 'normal_nucleoli', 'mitoses', 'tumor_class']\n",
    "\n",
    "#since ? cannot be converted to int we can convert all data to float\n",
    "wins_data[new_winsconsin_headers] = wins_data[new_winsconsin_headers].applymap(float)\n",
    "#print(wins_data.dtypes)\n",
    "\n",
    "#use the simple imputer function to replace missing value\n",
    "imputer = SimpleImputer (strategy = 'median') # replace most_frequent with median, mean and observe\n",
    "imputer.fit(wins_data)\n",
    "new_data = imputer.transform(wins_data)\n",
    "#reassign the new data frame\n",
    "wins_data = pd.DataFrame(new_data, columns=new_winsconsin_headers)\n",
    "\n",
    "#recheck the data for missing values\n",
    "win_empty_data = wins_data[wins_data.isna().any(axis=1)]\n",
    "#print('\\n These are the missing data \\n ', win_empty_data)\n",
    "\n",
    "#seperate the data into xtrain and y test groups  - training and target sets\n",
    "train_headers = ['c_thickness', 'uni_cell_size', 'uni_cell_shape', 'marg_adhesion', 'epi_cell_size', 'nuclei','bland_chromatin', 'normal_nucleoli', 'mitoses']\n",
    "target_header = ['tumor_class']\n",
    "\n",
    "X = wins_data[train_headers]\n",
    "y = wins_data[target_header]\n",
    "\n",
    "#split the data into train and test -- split  using 60:40\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1,stratify=y)\n",
    "\n",
    "#check the dimension of the train and test data\n",
    "print('\\n The total of training dataset', X_train.shape)\n",
    "print('\\n The total of test dataset', X_test.shape)\n",
    "\n",
    "#declare a seed variable to ensure reproducibility\n",
    "SEED =1\n",
    "#instantiate the model and set the hyperparameters\n",
    "#my_model = DecisionTreeClassifier(max_depth=4, min_samples_leaf=0.05, random_state=SEED, criterion = 'gini')#manually tuning our model\n",
    "my_model = DecisionTreeClassifier(random_state= SEED)\n",
    "print(\"\\n Hyperparameters of default model \\n\", my_model.get_params(), '\\n') #print the model default hyperparameters\n",
    "\n",
    "#train the model to fit\n",
    "my_model.fit(X_train, y_train)\n",
    "\n",
    "#create a cross validation split\n",
    "kfold_split = KFold(n_splits=10) # test b changing splits to 3, 5 and 10\n",
    "\n",
    "#declare a dictionary value of hyperparameters and values\n",
    "classifier_hyperpar = dict()\n",
    "classifier_hyperpar['max_depth'] = [2,3,4,6,8,10]\n",
    "classifier_hyperpar['min_samples_split'] = [2,4,6,8,9]\n",
    "classifier_hyperpar['min_samples_leaf'] = [0.05,0.2,0.5,1]\n",
    "classifier_hyperpar['criterion'] = ['gini', 'entropy']\n",
    "\n",
    "#perform a grid search and fit the grid\n",
    "classifier_grid = GridSearchCV(my_model,classifier_hyperpar,scoring='accuracy', n_jobs=-1,cv=kfold_split)\n",
    "classifier_grid_fit = classifier_grid.fit(X,y)\n",
    "\n",
    "#compute the array containing the 10 folds and calculate the cross validation mean score\n",
    "CV_scores = -cross_val_score(my_model, X_train, y_train, cv=kfold_split)\n",
    "#std for the computation range for this generalisation and mean to know how well the model generalises\n",
    "print(\"\\n Cross val mean: {:.3f} (std: {:.3f})\".format(CV_scores.mean()*-1, CV_scores.std()), end=\"\\n\\n\")\n",
    "\n",
    "#we can print the hyperparameter tuning results\n",
    "print('Best hyperparameters: %s' % classifier_grid_fit.best_params_)\n",
    "print('Best max_depth: = ',  classifier_grid_fit.best_estimator_.get_params()['max_depth'])\n",
    "print('Best min_samples_split: = ',  classifier_grid_fit.best_estimator_.get_params()['min_samples_split'])\n",
    "print('Best min_samples_leaf: = ',  classifier_grid_fit.best_estimator_.get_params()['min_samples_leaf'])\n",
    "print('Best criterion: = ',  classifier_grid_fit.best_estimator_.get_params()['criterion'])\n",
    "\n",
    "#print best hyperparameters\n",
    "print(\"suggested Best hyperparameters: \\n\", classifier_grid_fit.best_estimator_.get_params())\n",
    "\n",
    "print('Best score: %s {:.3f}\\n'.format(classifier_grid_fit.best_score_))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "RANDOM SEARCH CROSS VALIDATION TUNING"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(699, 11)\n",
      "(699, 10)\n",
      "\n",
      " The total of training dataset (419, 9)\n",
      "\n",
      " The total of test dataset (280, 9)\n",
      "\n",
      " Hyperparameters of default model \n",
      " {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': 1, 'splitter': 'best'} \n",
      "\n",
      "\n",
      " Cross val mean: 0.935 (std: 0.026)\n",
      "\n",
      "Best hyperparameters: {'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 4, 'criterion': 'gini'}\n",
      "Best max_depth: =  4\n",
      "Best min_samples_split: =  2\n",
      "Best min_samples_leaf: =  1\n",
      "Best criterion: =  gini\n",
      "suggested Best hyperparameters: \n",
      " {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 4, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': 1, 'splitter': 'best'}\n",
      "Best score: %s 0.937\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Randaom search tuning\n",
    "#Grid search model tuning\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from matplotlib import pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.model_selection import  train_test_split, KFold, cross_val_score,RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "#declaring header names\n",
    "winsconsin_headers = ['sample_code','c_thickness', 'uni_cell_size', 'uni_cell_shape', 'marg_adhesion', 'epi_cell_size', 'nuclei','bland_chromatin', 'normal_nucleoli', 'mitoses', 'tumor_class']\n",
    "\n",
    "#read the data using read_csv class of pandas\n",
    "wins_data = read_csv(\"venv/winsconsin_b_cancer (1).csv\" ,names= winsconsin_headers)\n",
    "\n",
    "print(wins_data.shape)\n",
    "\n",
    "wins_data.drop('sample_code', axis=1, inplace=True)\n",
    "print(wins_data.shape)\n",
    "#check all datas are numbers and convert any non-numeric characters to null value\n",
    "wins_data=wins_data.apply(pd.to_numeric, errors='coerce')\n",
    "#print(wins_data.apply(pd.to_numeric, errors='coerce').info()) #this will give the datatype info after the conversion\n",
    "\n",
    "#declaring a new header\n",
    "new_winsconsin_headers = ['c_thickness', 'uni_cell_size', 'uni_cell_shape', 'marg_adhesion', 'epi_cell_size', 'nuclei','bland_chromatin', 'normal_nucleoli', 'mitoses', 'tumor_class']\n",
    "\n",
    "#since ? cannot be converted to int we can convert all data to float\n",
    "wins_data[new_winsconsin_headers] = wins_data[new_winsconsin_headers].applymap(float)\n",
    "#print(wins_data.dtypes)\n",
    "\n",
    "#use the simple imputer function to replace missing value\n",
    "imputer = SimpleImputer (strategy = 'median') # replace most_frequent with median, mean and observe\n",
    "imputer.fit(wins_data)\n",
    "new_data = imputer.transform(wins_data)\n",
    "#reassign the new data frame\n",
    "wins_data = pd.DataFrame(new_data, columns=new_winsconsin_headers)\n",
    "\n",
    "#recheck the data for missing values\n",
    "win_empty_data = wins_data[wins_data.isna().any(axis=1)]\n",
    "#print('\\n These are the missing data \\n ', win_empty_data)\n",
    "\n",
    "#seperate the data into xtrain and y test groups  - training and target sets\n",
    "train_headers = ['c_thickness', 'uni_cell_size', 'uni_cell_shape', 'marg_adhesion', 'epi_cell_size', 'nuclei','bland_chromatin', 'normal_nucleoli', 'mitoses']\n",
    "target_header = ['tumor_class']\n",
    "\n",
    "X = wins_data[train_headers]\n",
    "y = wins_data[target_header]\n",
    "\n",
    "#split the data into train and test -- split  using 60:40\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1,stratify=y)\n",
    "\n",
    "#check the dimension of the train and test data\n",
    "print('\\n The total of training dataset', X_train.shape)\n",
    "print('\\n The total of test dataset', X_test.shape)\n",
    "\n",
    "#declare a seed variable to ensure reproducibility\n",
    "SEED =1\n",
    "#instantiate the model and set the hyperparameters\n",
    "#my_model = DecisionTreeClassifier(max_depth=4, min_samples_leaf=0.05, random_state=SEED, criterion = 'gini')#manually tuning our model\n",
    "my_model = DecisionTreeClassifier(random_state= SEED)\n",
    "print(\"\\n Hyperparameters of default model \\n\", my_model.get_params(), '\\n') #print the model default hyperparameters\n",
    "\n",
    "#train the model to fit\n",
    "my_model.fit(X_train, y_train)\n",
    "\n",
    "#create a cross validation split\n",
    "kfold_split = KFold(n_splits=10) # test b changing splits to 3, 5 and 10\n",
    "\n",
    "#declare a dictionary value of hyperparameters and values\n",
    "classifier_hyperpar = dict()\n",
    "classifier_hyperpar['max_depth'] = [2,3,4,6,8,10]\n",
    "classifier_hyperpar['min_samples_split'] = [2,4,6,8,9]\n",
    "classifier_hyperpar['min_samples_leaf'] = [0.05,0.2,0.5,1]\n",
    "classifier_hyperpar['criterion'] = ['gini', 'entropy']\n",
    "\n",
    "#perform a grid search and fit the grid\n",
    "classifier_grid = RandomizedSearchCV(my_model,classifier_hyperpar,scoring='accuracy', n_jobs=-1,cv=kfold_split)\n",
    "classifier_grid_fit = classifier_grid.fit(X,y)\n",
    "\n",
    "#compute the array containing the 10 folds and calculate the cross validation mean score\n",
    "CV_scores = -cross_val_score(my_model, X_train, y_train, cv=kfold_split)\n",
    "#std for the computation range for this generalisation and mean to know how well the model generalises\n",
    "print(\"\\n Cross val mean: {:.3f} (std: {:.3f})\".format(CV_scores.mean()*-1, CV_scores.std()), end=\"\\n\\n\")\n",
    "\n",
    "#we can print the hyperparameter tuning results\n",
    "print('Best hyperparameters: %s' % classifier_grid_fit.best_params_)\n",
    "print('Best max_depth: = ',  classifier_grid_fit.best_estimator_.get_params()['max_depth'])\n",
    "print('Best min_samples_split: = ',  classifier_grid_fit.best_estimator_.get_params()['min_samples_split'])\n",
    "print('Best min_samples_leaf: = ',  classifier_grid_fit.best_estimator_.get_params()['min_samples_leaf'])\n",
    "print('Best criterion: = ',  classifier_grid_fit.best_estimator_.get_params()['criterion'])\n",
    "\n",
    "#print best hyperparameters\n",
    "print(\"suggested Best hyperparameters: \\n\", classifier_grid_fit.best_estimator_.get_params())\n",
    "\n",
    "print('Best score: %s {:.3f}\\n'.format(classifier_grid_fit.best_score_))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "    ENSEMBLE LEARNING"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(699, 11)\n",
      "(699, 10)\n",
      "\n",
      " The total of training dataset (489, 9)\n",
      "\n",
      " The total of test dataset (210, 9)\n",
      "\n",
      " Cross Val mean: 0.971 (std: 0.026)\n",
      "\n",
      "\n",
      "  LogisticRegression: Test : 0.943\n",
      "\n",
      "  LogisticRegression: Train : 0.973\n",
      "____________---------------------_______________\n",
      "\n",
      " Cross Val mean: 0.977 (std: 0.026)\n",
      "\n",
      "\n",
      "  K Nearest Neighbour:  Test : 0.948\n",
      "\n",
      "  K Nearest Neighbour:  Train : 0.984\n",
      "____________---------------------_______________\n",
      "\n",
      " Cross Val mean: 0.949 (std: 0.026)\n",
      "\n",
      "\n",
      "  DecisionTreeClassifier:  Test : 0.948\n",
      "\n",
      "  DecisionTreeClassifier:  Train : 1.000\n",
      "____________---------------------_______________\n",
      "\n",
      " Cross Val mean: 0.973 (std: 0.026)\n",
      "\n",
      "\n",
      " Voting Classifier Train 0.986\n"
     ]
    }
   ],
   "source": [
    "#Ensemble - Hard voting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from matplotlib import pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.model_selection import  train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "#importing the necessary models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "\n",
    "\n",
    "#declaring header names\n",
    "winsconsin_headers = ['sample_code','c_thickness', 'uni_cell_size', 'uni_cell_shape', 'marg_adhesion', 'epi_cell_size', 'nuclei','bland_chromatin', 'normal_nucleoli', 'mitoses', 'tumor_class']\n",
    "\n",
    "#read the data using read_csv class of pandas\n",
    "wins_data = read_csv(\"venv/winsconsin_b_cancer (1).csv\" ,names= winsconsin_headers)\n",
    "\n",
    "print(wins_data.shape)\n",
    "\n",
    "wins_data.drop('sample_code', axis=1, inplace=True)\n",
    "print(wins_data.shape)\n",
    "#check all datas are numbers and convert any non-numeric characters to null value\n",
    "wins_data=wins_data.apply(pd.to_numeric, errors='coerce')\n",
    "#print(wins_data.apply(pd.to_numeric, errors='coerce').info()) #this will give the datatype info after the conversion\n",
    "\n",
    "#declaring a new header\n",
    "new_winsconsin_headers = ['c_thickness', 'uni_cell_size', 'uni_cell_shape', 'marg_adhesion', 'epi_cell_size', 'nuclei','bland_chromatin', 'normal_nucleoli', 'mitoses', 'tumor_class']\n",
    "\n",
    "#since ? cannot be converted to int we can convert all data to float\n",
    "wins_data[new_winsconsin_headers] = wins_data[new_winsconsin_headers].applymap(float)\n",
    "#print(wins_data.dtypes)\n",
    "\n",
    "#use the simple imputer function to replace missing value\n",
    "imputer = SimpleImputer (strategy = 'median') # replace most_frequent with median, mean and observe\n",
    "imputer.fit(wins_data)\n",
    "new_data = imputer.transform(wins_data)\n",
    "#reassign the new data frame\n",
    "wins_data = pd.DataFrame(new_data, columns=new_winsconsin_headers)\n",
    "\n",
    "#recheck the data for missing values\n",
    "win_empty_data = wins_data[wins_data.isna().any(axis=1)]\n",
    "#print('\\n These are the missing data \\n ', win_empty_data)\n",
    "\n",
    "#seperate the data into xtrain and y test groups  - training and target sets\n",
    "X, y = new_data[:, :-1], new_data[:, -1]\n",
    "\n",
    "#declare a seed variable to ensure reproducibility\n",
    "SEED =1\n",
    "#split the data into train and test -- split  using 60:40\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1,stratify=y)\n",
    "\n",
    "#check the dimension of the train and test data\n",
    "print('\\n The total of training dataset', X_train.shape)\n",
    "print('\\n The total of test dataset', X_test.shape)\n",
    "\n",
    "#instantiate the models\n",
    "lr = LogisticRegression(random_state=SEED)\n",
    "knc = KNN()\n",
    "dtc = DecisionTreeClassifier(random_state=SEED)\n",
    "\n",
    "classifier_list = [('LogisticRegression:', lr),('K Nearest Neighbour: ', knc), ('DecisionTreeClassifier: ', dtc)]\n",
    "\n",
    "#instantiate the cross validation cv\n",
    "kfold_split = KFold(n_splits=10)\n",
    "\n",
    "\n",
    "#a for loop to iterate through the models\n",
    "for clsf_name, clsf in classifier_list:\n",
    "\n",
    "    #fit each model\n",
    "    clsf.fit(X_train, y_train)\n",
    "\n",
    "    #computethe array containing the 10 folds\n",
    "    CV_scores_clsf = -cross_val_score(clsf, X_train, y_train, cv=kfold_split)\n",
    "    print(\"\\n Cross Val mean: {:.3f} (std: {:.3f})\".format(CV_scores_clsf.mean()*-1, CV_scores.std()), end='\\n\\n' )\n",
    "\n",
    "    #predict and calculate the accuracy on test data for each model\n",
    "    y_pred_test_clsf = clsf.predict(X_test)\n",
    "    print('\\n  {:s} Test : {:.3f}'.format(clsf_name, accuracy_score(y_test, y_pred_test_clsf)))\n",
    "\n",
    "    #predict and calculate the accuracy on train data for each model\n",
    "    y_pred_train_clsf = clsf.predict(X_train)\n",
    "    print('\\n  {:s} Train : {:.3f}'.format(clsf_name, accuracy_score(y_train, y_pred_train_clsf)))\n",
    "\n",
    "    print('____________---------------------_______________')\n",
    "\n",
    "#instantiate the voting classifier\n",
    "vc = VotingClassifier(estimators=classifier_list)\n",
    "\n",
    "#fit vc to the training sets and models\n",
    "vc.fit(X_train, y_train)\n",
    "\n",
    "#computethe array containing the 10 folds cv mses\n",
    "CV_scores_vc = -cross_val_score(vc, X_train, y_train, cv=10)\n",
    "print(\"\\n Cross Val mean: {:.3f} (std: {:.3f})\".format(CV_scores_vc.mean()*-1, CV_scores.std()), end='\\n\\n' )\n",
    "\n",
    "#predict the label for training set vc\n",
    "y_pred_vc_train = vc.predict(X_train)\n",
    "print('\\n Voting Classifier Train {:.3f}'.format(accuracy_score(y_train, y_pred_vc_train)))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
